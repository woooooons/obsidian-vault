## FP16 / INT8

* **정의:** FP16, INT8은 딥러닝 모델에서 **연산 및 저장에 사용되는 숫자 표현 방식(정밀도)** 을 의미함.
* **목적:** 모델의 **속도 향상**, **메모리 사용량 감소**, **추론 효율 개선**
* **적용 위치:** GPU 연산, 딥러닝 학습 및 추론 단계

---

## 1. **FP16 (Half-precision Floating Point)**

* **정식 명칭:** 16-bit Floating Point
* **비트 구성:**

  * 부호(Sign): 1bit
  * 지수(Exponent): 5bit
  * 가수(Mantissa): 10bit
* **사용 범위:** 모델 학습, 추론

### 특징

* FP32 대비 **메모리 사용량 50% 감소**
* GPU(Tensor Core)에서 연산 시 **처리 속도 크게 향상**
* 표현 정밀도 감소로 인해

  * Gradient Underflow / Overflow 발생 가능

### 용도

* 대규모 딥러닝 모델 학습
* GPU 메모리 제약 환경
* 고속 추론이 필요한 서버 환경

---

## 2. **INT8 (8-bit Integer)**

* **정식 명칭:** 8-bit Integer
* **표현 범위:** -128 ~ 127
* **사용 범위:** 추론(Inference) 전용

### 특징

* 정수 기반 연산 → **연산 단순, 처리 속도 빠름**
* FP32 대비 **메모리 사용량 최대 4배 절감**
* 부동소수점 → 정수 변환 과정에서

  * **정밀도 손실 발생**
  * 양자화(Quantization) 필수

### 용도

* 모바일, 임베디드, 엣지 디바이스
* 실시간 영상 분석, 다중 스트림 처리
* TensorRT 기반 고속 추론

---

## 3. 정밀도 비교

| 정밀도  | 비트 수  | 표현 범위      | 장점            | 단점        | 주요 용도 |
| ---- | ----- | ---------- | ------------- | --------- | ----- |
| FP32 | 32bit | ±3.4e38    | 높은 정밀도        | 메모리·연산 부담 | 학습 기본 |
| FP16 | 16bit | ±6.5e4     | 메모리 절약, 연산 가속 | 정밀도 감소    | 학습·추론 |
| INT8 | 8bit  | -128 ~ 127 | 최고 속도, 최소 메모리 | 정밀도 손실    | 추론 전용 |

---

## 4. 요약 정리

* **FP16**

  * 부동소수점 반정밀도
  * 학습과 추론 모두 사용 가능
  * GPU 연산 가속에 효과적

* **INT8**

  * 정수 기반 초저정밀도
  * 추론 전용 최적화
  * 메모리·전력 효율 최우선 환경에 적합

* 공통 목적
  → **딥러닝 모델을 더 작고 빠르게 만들기 위한 핵심 최적화 수단**

---

## 실무 팁

* YOLO, OCR 서버 환경
  → FP16 + TensorRT 조합이 가장 균형 좋음
* 모바일·엣지 디바이스
  → INT8 양자화 필수
* 학습 단계
  → FP32 또는 Mixed Precision(FP16 + FP32)


---
